# SmolLM Pretraining Configuration - 160M Model
# Faster training, good for testing and iteration

# Model architecture (~160M params)
model:
  d_model: 768           # Half of 1536
  n_layers: 18           # Slightly more layers for ~160M
  n_heads: 12            # 768 / 12 = 64 head_dim
  n_kv_heads: 4          # GQA: 3:1 ratio
  ffn_dim: 2048         # ~2.67x expansion
  vocab_size: 32000
  max_seq_len: 1024
  rope_theta: 10000.0
  nope_layer_interval: 4
  dropout: 0.0
  attention_dropout: 0.0
  use_flash_attn: true
  init_std_factor: 0.5
  scale_embeddings: true
  tie_embeddings: true

# Tokenizer
tokenizer:
  name: "openlm-research/open_llama_3b"

# Training configuration (can use larger batch with smaller model)
training:
  total_tokens: 5_000_000_000    # 5B tokens
  micro_batch_size: 32
  seq_len: 1024
  gradient_accumulation: 48       # Effective: 32 * 48 * 2048 = 3.15M tokens/step
  precision: "bf16"

# Optimizer
optimizer:
  muon_lr: 0.02
  adam_lr: 0.0003
  weight_decay: 0.1
  grad_clip: 1.0
  muon_momentum: 0.95
  adam_betas: [0.9, 0.95]

# Scheduler
scheduler:
  warmup_steps: 200   # ~12.5% of total ~1.6k steps for 5B tokens
  decay_ratio: 0.1
  min_lr_ratio: 0.1

# Data mix
data:
  web:
    weight: 0.87
    sources:
      - name: "HuggingFaceFW/fineweb-edu"
        weight: 0.5
      - name: "mlfoundations/dclm-baseline-1.0"
        weight: 0.5
  code:
    weight: 0.10
    sources:
      - name: "codeparrot/codeparrot-clean"
        weight: 1.0
    text_field: "content"
  math:
    weight: 0.03
    sources:
      - name: "open-web-math/open-web-math"
        weight: 1.0

# Validation
validation:
  eval_every_steps: 100
  web:
    num_samples: 5000
    seed: 42
  code:
    num_samples: 2000
    seed: 42
  math:
    num_samples: 1000
    seed: 42

# Logging
logging:
  log_every_steps: 10
  eval_every_steps: 100
  project: "smol-lm-160m"
  use_wandb: true
  log_dir: "./logs"

# Checkpointing
checkpoint:
  save_every_steps: 500
  save_every_hours: 2.0
  checkpoint_dir: "/content/drive/MyDrive/smol-lm-checkpoints"
  keep_last_n: 3
